<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Running StatefulSets in Production - Suheb Ghare</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <nav>
            <div class="logo">Suheb Ghare</div>
            <div class="nav-links">
                <a href="index.html">Home</a>
                <a href="index.html#blog-index">Blog Index</a>
                <a href="https://portfolio.suhebghare.tech" target="_blank">Portfolio</a>
            </div>
        </nav>
    </header>

    <main>
        <a href="index.html" class="back-link">‚Üê Back to Home</a>
        
        <article class="blog-content">
            <div class="blog-header">
                <h1>Real Lessons from Running StatefulSets in Production</h1>
                <div class="blog-meta">Published on September 28, 2024 | 15 min read</div>
            </div>


            <div class="blog-stats">
                <div class="stat-item">
                    <span class="icon">üëÅÔ∏è</span>
                    <span class="count reads-count">855</span>
                    <span class="label">reads</span>
                </div>
                <div class="stat-item">
                    <span class="icon">üëç</span>
                    <span class="count likes-count">414</span>
                    <span class="label">likes</span>
                </div>
                <div class="stat-item">
                    <span class="icon">üëé</span>
                    <span class="count dislikes-count">39</span>
                    <span class="label">dislikes</span>
                </div>
            </div>

            <div class="blog-actions">
                <button class="like-btn" onclick="blogStats.like('statefulsets-production-lessons')">üëç Like</button>
                <button class="dislike-btn" onclick="blogStats.dislike('statefulsets-production-lessons')">üëé Dislike</button>
            </div>
            <h2>The Hard Truth About StatefulSets</h2>
            <p>After running StatefulSets in production for 2 years (PostgreSQL, Redis, Kafka, Elasticsearch), here are the painful lessons we learned.</p>

            <div class="info-box danger">
                <strong>Reality Check:</strong> StatefulSets are NOT just "Deployments with persistent storage." They require fundamentally different operational practices.
            </div>

            <h2>Our StatefulSet Journey</h2>

            <div class="grid-2">
                <div class="metric-card">
                    <h4>Incidents Year 1</h4>
                    <div class="metric-value">23</div>
                    <p>StatefulSet-related outages</p>
                </div>
                <div class="metric-card">
                    <h4>Incidents Year 2</h4>
                    <div class="metric-value">2</div>
                    <p>After implementing lessons learned</p>
                </div>
            </div>

            <h2>Lesson 1: Pod Identity Matters (A Lot)</h2>

            <div class="info-box warning">
                <strong>The Mistake:</strong> Treating StatefulSet pods as interchangeable like Deployment pods
            </div>

            <h3>What We Learned</h3>

            <pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres-headless
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:14
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # Pod name is stable: postgres-0, postgres-1, postgres-2
        # DNS: postgres-0.postgres-headless.default.svc.cluster.local</code></pre>

            <div class="info-box success">
                <strong>Key Insight:</strong> Each pod has a stable network identity. Use this for leader election, replication, and cluster formation.
            </div>

            <h2>Lesson 2: Storage is Permanent (Even When You Don't Want It)</h2>

            <h3>The PVC Retention Problem</h3>

            <div class="timeline">
                <div class="timeline-item">
                    <strong>Day 1:</strong> Deleted StatefulSet to fix config issue
                </div>
                <div class="timeline-item">
                    <strong>Day 1 + 5min:</strong> Recreated StatefulSet with new config
                </div>
                <div class="timeline-item">
                    <strong>Day 1 + 10min:</strong> Pods stuck in CrashLoopBackOff
                </div>
                <div class="timeline-item">
                    <strong>Root Cause:</strong> Old PVCs with corrupted data still attached
                </div>
            </div>

            <h3>PVC Management Strategy</h3>

            <pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: postgres
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: gp3-encrypted
      resources:
        requests:
          storage: 100Gi

# PVCs are NOT deleted when StatefulSet is deleted!
# Manual cleanup required:
# kubectl delete pvc data-postgres-0 data-postgres-1 data-postgres-2</code></pre>

            <div class="info-box danger">
                <strong>Critical:</strong> Always backup before deleting StatefulSets. PVCs persist and will reattach to new pods with potentially incompatible data.
            </div>

            <h2>Lesson 3: Scaling is NOT Like Deployments</h2>

            <h3>The Scaling Incident</h3>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Action</th>
                        <th>Deployment Behavior</th>
                        <th>StatefulSet Behavior</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Scale Up</td>
                        <td>All pods created simultaneously</td>
                        <td>Pods created one at a time, sequentially</td>
                    </tr>
                    <tr>
                        <td>Scale Down</td>
                        <td>Random pods terminated</td>
                        <td>Highest ordinal terminated first (reverse order)</td>
                    </tr>
                    <tr>
                        <td>Speed</td>
                        <td>Fast (parallel)</td>
                        <td>Slow (sequential)</td>
                    </tr>
                </tbody>
            </table>

            <h3>Safe Scaling Procedure</h3>

            <pre><code># Scale up PostgreSQL from 3 to 5 replicas
# Step 1: Verify current state
kubectl get statefulset postgres
kubectl get pvc | grep postgres

# Step 2: Scale up
kubectl scale statefulset postgres --replicas=5

# Step 3: Wait for postgres-3 to be ready
kubectl wait --for=condition=ready pod/postgres-3 --timeout=300s

# Step 4: Verify replication before postgres-4 starts
kubectl exec postgres-3 -- psql -c "SELECT * FROM pg_stat_replication;"

# Step 5: Monitor postgres-4 creation
kubectl get pods -w | grep postgres-4</code></pre>

            <div class="info-box warning">
                <strong>Scaling Down:</strong> Always verify data replication before scaling down. StatefulSet will delete highest ordinal first.
            </div>

            <h2>Lesson 4: Updates Require Careful Planning</h2>

            <h3>Update Strategies</h3>

            <pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # Update all pods
      maxUnavailable: 1  # Only one pod down at a time
  
  podManagementPolicy: OrderedReady  # Default, sequential updates</code></pre>

            <h3>Our Update Process</h3>

            <div class="timeline">
                <div class="timeline-item">
                    <strong>Step 1:</strong> Set partition to total replicas (no updates)
                </div>
                <div class="timeline-item">
                    <strong>Step 2:</strong> Update StatefulSet spec
                </div>
                <div class="timeline-item">
                    <strong>Step 3:</strong> Decrease partition by 1, update last pod
                </div>
                <div class="timeline-item">
                    <strong>Step 4:</strong> Verify pod health, check replication
                </div>
                <div class="timeline-item">
                    <strong>Step 5:</strong> Repeat for each pod
                </div>
            </div>

            <pre><code># Canary update: Update only postgres-2 first
kubectl patch statefulset postgres -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":2}}}}'

# Apply new image
kubectl set image statefulset/postgres postgres=postgres:15

# Only postgres-2 will update
# Verify it works before updating others

# Update postgres-1
kubectl patch statefulset postgres -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":1}}}}'

# Update all
kubectl patch statefulset postgres -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":0}}}}'</code></pre>

            <h2>Lesson 5: Node Affinity is Critical</h2>

            <div class="info-box danger">
                <strong>The Outage:</strong> All 3 PostgreSQL pods scheduled on same node. Node failure = complete database outage.
            </div>

            <h3>Proper Anti-Affinity Configuration</h3>

            <pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  template:
    spec:
      affinity:
        # REQUIRED: Spread across nodes
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - postgres
            topologyKey: kubernetes.io/hostname
        
        # PREFERRED: Spread across AZs
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - postgres
              topologyKey: topology.kubernetes.io/zone
        
        # REQUIRED: Only schedule on OnDemand nodes
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: lifecycle
                operator: In
                values:
                - ondemand</code></pre>

            <h2>Lesson 6: Backup Strategy is Non-Negotiable</h2>

            <h3>Our Backup Approach</h3>

            <div class="grid-2">
                <div class="info-box success">
                    <h4>What We Do</h4>
                    <ul class="icon-list">
                        <li>Automated daily snapshots (EBS)</li>
                        <li>Logical backups to S3 (pg_dump)</li>
                        <li>Point-in-time recovery enabled</li>
                        <li>Monthly restore tests</li>
                        <li>Cross-region replication</li>
                    </ul>
                </div>
                <div class="info-box danger">
                    <h4>What We Learned</h4>
                    <ul class="icon-list cross">
                        <li>PVC snapshots alone aren't enough</li>
                        <li>Test restores, don't assume they work</li>
                        <li>Document restore procedures</li>
                        <li>Automate backup verification</li>
                    </ul>
                </div>
            </div>

            <h3>Automated Backup CronJob</h3>

            <pre><code>apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
spec:
  schedule: "0 2 * * *"  # 2 AM daily
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:14
            command:
            - /bin/bash
            - -c
            - |
              TIMESTAMP=$(date +%Y%m%d_%H%M%S)
              pg_dump -h postgres-0.postgres-headless \
                      -U postgres \
                      -d production \
                      | gzip > /backup/postgres_${TIMESTAMP}.sql.gz
              
              aws s3 cp /backup/postgres_${TIMESTAMP}.sql.gz \
                        s3://backups/postgres/
              
              # Keep only last 30 days
              find /backup -name "postgres_*.sql.gz" -mtime +30 -delete
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
            volumeMounts:
            - name: backup
              mountPath: /backup
          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure</code></pre>

            <h2>Lesson 7: Monitoring StatefulSets is Different</h2>

            <h3>Key Metrics to Track</h3>

            <div class="info-box">
                <ul class="icon-list">
                    <li>Pod readiness per ordinal (not just total)</li>
                    <li>PVC usage per pod</li>
                    <li>Replication lag (for databases)</li>
                    <li>Pod restart count per ordinal</li>
                    <li>Time to ready after restart</li>
                </ul>
            </div>

            <h3>Prometheus Alerts</h3>

            <pre><code>groups:
- name: statefulset_alerts
  rules:
  - alert: StatefulSetPodDown
    expr: kube_statefulset_status_replicas_ready < kube_statefulset_status_replicas
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "StatefulSet {{ $labels.statefulset }} has pods down"
  
  - alert: StatefulSetPVCNearFull
    expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.85
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "PVC {{ $labels.persistentvolumeclaim }} is 85% full"
  
  - alert: StatefulSetUpdateStuck
    expr: kube_statefulset_status_current_revision != kube_statefulset_status_update_revision
    for: 30m
    labels:
      severity: warning
    annotations:
      summary: "StatefulSet {{ $labels.statefulset }} update stuck"</code></pre>

            <h2>Lesson 8: Disaster Recovery Procedures</h2>

            <h3>Complete Cluster Loss Scenario</h3>

            <div class="timeline">
                <div class="timeline-item">
                    <strong>Step 1:</strong> Restore PVCs from EBS snapshots
                </div>
                <div class="timeline-item">
                    <strong>Step 2:</strong> Create StatefulSet with same name
                </div>
                <div class="timeline-item">
                    <strong>Step 3:</strong> Pods attach to restored PVCs automatically
                </div>
                <div class="timeline-item">
                    <strong>Step 4:</strong> Verify data integrity
                </div>
                <div class="timeline-item">
                    <strong>Step 5:</strong> Resume application traffic
                </div>
            </div>

            <h2>Common Mistakes to Avoid</h2>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Mistake</th>
                        <th>Impact</th>
                        <th>Solution</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Running on Spot instances</td>
                        <td>Data loss, corruption</td>
                        <td>OnDemand only for StatefulSets</td>
                    </tr>
                    <tr>
                        <td>No PodDisruptionBudget</td>
                        <td>Multiple pods down during updates</td>
                        <td>Set minAvailable: 2</td>
                    </tr>
                    <tr>
                        <td>Insufficient resources</td>
                        <td>Pods stuck pending</td>
                        <td>Reserve resources with requests</td>
                    </tr>
                    <tr>
                        <td>No anti-affinity</td>
                        <td>All pods on same node</td>
                        <td>Required anti-affinity rules</td>
                    </tr>
                    <tr>
                        <td>Deleting PVCs manually</td>
                        <td>Permanent data loss</td>
                        <td>Always backup first</td>
                    </tr>
                </tbody>
            </table>

            <h2>Production-Ready StatefulSet Template</h2>

            <pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: production
spec:
  serviceName: postgres-headless
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  
  podManagementPolicy: OrderedReady
  
  template:
    metadata:
      labels:
        app: postgres
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: postgres
            topologyKey: kubernetes.io/hostname
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: lifecycle
                operator: In
                values:
                - ondemand
      
      containers:
      - name: postgres
        image: postgres:14
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        
        livenessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - postgres
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - postgres
          initialDelaySeconds: 5
          periodSeconds: 5
        
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: gp3-encrypted
      resources:
        requests:
          storage: 100Gi</code></pre>

            <h2>Key Takeaways</h2>

            <div class="info-box success">
                <ul class="icon-list">
                    <li>StatefulSets require OnDemand nodes only</li>
                    <li>Always use pod anti-affinity</li>
                    <li>PVCs persist after StatefulSet deletion</li>
                    <li>Scaling is sequential, not parallel</li>
                    <li>Use partition for canary updates</li>
                    <li>Backup and test restore procedures</li>
                    <li>Monitor per-pod metrics, not just totals</li>
                    <li>Document disaster recovery procedures</li>
                </ul>
            </div>

            <h2>Conclusion</h2>
            <p>StatefulSets are powerful but unforgiving. Our 23 incidents in year one taught us that StatefulSets require fundamentally different operational practices than Deployments. The key lessons: use OnDemand nodes, implement proper anti-affinity, backup religiously, and never rush updates. With these practices, we reduced incidents by 91% in year two.</p>
        </article>
    </main>

    <footer>
        <p>&copy; 2024 Suheb Ghare. All rights reserved.</p>
    </footer>

    <script src="blog-stats.js"></script>
    <script>
        window.addEventListener('DOMContentLoaded', function() {
            blogStats.updateDisplay('statefulsets-production-lessons');
            blogStats.incrementRead('statefulsets-production-lessons');
        });
    </script>
</body>
</html>
